{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUU87MT26hu4JgfRrnG6TP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoelByron/AI_Camp_TalentoTech/blob/main/notebooks/09_iIntroduccion_Keras_en_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo de Tokenización\n",
        "\n",
        "con keras"
      ],
      "metadata": {
        "id": "C7dIMTHJJAqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "metadata": {
        "id": "rj0jA4brI_vH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZqUYJctIg7e"
      },
      "outputs": [],
      "source": [
        "frases = ['Hola mundo', 'Hola a todos', 'Hola a todo el mundo ']\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Genera el diccionario de tokens\n",
        "tokenizar = tf.keras.preprocessing.text.Tokenizer(num_words=10) # 10 palabras más comunes\n",
        "tokenizar.fit_on_texts(frases) # Crea el diccionario\n",
        "word_index = tokenizar.word_index\n",
        "\n",
        "print('word_index =', word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNwLPdSoJZd1",
        "outputId": "77a4923c-1063-475b-e7fe-cc61e0b1b0b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word_index = {'hola': 1, 'mundo': 2, 'a': 3, 'todos': 4, 'todo': 5, 'el': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generación de secuencia tokenizadas\n",
        "\n",
        "secuencias = tokenizar.texts_to_sequences(frases) # convierte fraces a secuencia de tokens\n",
        "print('secuencias =', secuencias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2Kaee8_K0YD",
        "outputId": "4d4bbfc6-fb20-4d84-d041-b56ba7d65527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "secuencias = [[1, 2], [1, 3, 4], [1, 3, 5, 6, 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rellena las secuencias a una longitud uniforme\n",
        "relleno = tf.keras.preprocessing.sequence.pad_sequences(secuencias) # rellena con 0 hasta alcanzar la sencuia de la longitud mas larga\n",
        "print('relleno =', relleno)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4RShVC1K7M4",
        "outputId": "2123e21e-34e0-4fd7-a033-7e306bb0b8da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relleno = [[0 0 0 1 2]\n",
            " [0 0 1 3 4]\n",
            " [1 3 5 6 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "frases = ['Hola mundo', 'Hola a todos', 'Hola a todo el mundo ','Buen día, comó estas hoy?']"
      ],
      "metadata": {
        "id": "2OqvW5qENlOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Genera el diccionario de tokens\n",
        "tokenizar = tf.keras.preprocessing.text.Tokenizer(num_words=10) # 10 palabras más comunes\n",
        "tokenizar.fit_on_texts(frases) # Crea el diccionario\n",
        "word_index = tokenizar.word_index\n",
        "\n",
        "print('word_index =', word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "menzCvHRN27B",
        "outputId": "6dfdc20d-1c6f-4425-e776-1528012b4d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word_index = {'hola': 1, 'mundo': 2, 'a': 3, 'todos': 4, 'todo': 5, 'el': 6, 'buen': 7, 'día': 8, 'comó': 9, 'estas': 10, 'hoy': 11}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generación de secuencia tokenizadas\n",
        "\n",
        "secuencias = tokenizar.texts_to_sequences(frases) # convierte fraces a secuencia de tokens\n",
        "print('secuencias =', secuencias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuK_beLGN78P",
        "outputId": "da624cb0-3691-41d5-f884-5b301da4368c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "secuencias = [[1, 2], [1, 3, 4], [1, 3, 5, 6, 2], [7, 8, 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rellena las secuencias a una longitud uniforme\n",
        "relleno = tf.keras.preprocessing.sequence.pad_sequences(secuencias) # rellena con 0 hasta alcanzar la sencuia de la longitud mas larga\n",
        "print('relleno =\\n', relleno)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRZV4aslNzZI",
        "outputId": "41bc635c-0138-4b51-dcf8-d855ec887b81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relleno =\n",
            " [[0 0 0 1 2]\n",
            " [0 0 1 3 4]\n",
            " [1 3 5 6 2]\n",
            " [0 0 7 8 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo de Tokenización\n",
        "\n",
        "con keras"
      ],
      "metadata": {
        "id": "tVGOgZJPKz76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frases = ['Hola mundo', 'Hola a todos', 'Hola a todo el mundo ','Buen día, comó estas hoy?']"
      ],
      "metadata": {
        "id": "p1CaABPWL4nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Genera el diccionario de tokens\n",
        "tokenizar = tf.keras.preprocessing.text.Tokenizer(num_words=10, oov_token=\"<OOV>\") # 10 palabras más comunes\n",
        "tokenizar.fit_on_texts(frases) # Crea el diccionario\n",
        "word_index = tokenizar.word_index\n",
        "\n",
        "print('word_index =', word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EN2EEiiYMDHd",
        "outputId": "d6777eff-0a8c-4d8b-8e27-d360aa0424fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word_index = {'<OOV>': 1, 'hola': 2, 'mundo': 3, 'a': 4, 'todos': 5, 'todo': 6, 'el': 7, 'buen': 8, 'día': 9, 'comó': 10, 'estas': 11, 'hoy': 12}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generación de secuencia tokenizadas\n",
        "\n",
        "secuencias = tokenizar.texts_to_sequences(frases) # convierte fraces a secuencia de tokens\n",
        "print('secuencias =', secuencias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SqKgUB8NH3b",
        "outputId": "fb7e3449-922c-4a79-cd85-340c28eb21ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "secuencias = [[2, 3], [2, 4, 5], [2, 4, 6, 7, 3], [8, 9, 1, 1, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rellena las secuencias a una longitud uniforme\n",
        "relleno = tf.keras.preprocessing.sequence.pad_sequences(secuencias) # rellena con 0 hasta alcanzar la sencuia de la longitud mas larga\n",
        "print('relleno =', relleno)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfQWIZVaNKrB",
        "outputId": "43e166a7-4cc3-4c36-cf1f-239c8b6f03d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relleno = [[0 0 0 2 3]\n",
            " [0 0 2 4 5]\n",
            " [2 4 6 7 3]\n",
            " [8 9 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#'pre': Trunca la secuencia desde el principio, es decir, elimina los elementos de la secuencia al principio.\n",
        "#'post': Trunca la secuencia desde el final, es decir, elimina los elementos de la secuencia al final\n",
        "\n",
        "# Rellena las secuencias a una longitud uniforme\n",
        "relleno = tf.keras.preprocessing.sequence.pad_sequences(secuencias,padding = 'post', truncating = 'post') # rellena con 0 hasta alcanzar la sencuia de la longitud mas larga\n",
        "print('relleno =', relleno)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87Wk2oiHNVdq",
        "outputId": "41791f75-4765-4417-a982-1e7a35ebf2eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relleno = [[2 3 0 0 0]\n",
            " [2 4 5 0 0]\n",
            " [2 4 6 7 3]\n",
            " [8 9 1 1 1]]\n"
          ]
        }
      ]
    }
  ]
}